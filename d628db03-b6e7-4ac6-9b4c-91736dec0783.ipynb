{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keterangan Tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Film Junky Union, sebuah komunitas baru bagi penggemar film klasik sedang mengembangkan sistem untuk memfilter dan mengategorikan ulasan film. Misi utamanya adalah melatih model agar bisa mendeteksi ulasan negatif secara otomatis. Anda akan menggunakan *dataset* ulasan film IMBD dengan pelabelan polaritas untuk membuat sebuah model yang bisa mengklasifikasikan ulasan positif dan negatif. Model ini setidaknya harus memiliki skor F1 sebesar 0,85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inisialisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# baris berikutnya menyediakan grafik dengan kualitas yang lebih baik di layar HiDPI \n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini untuk menggunakan progress_apply, baca lebih lanjut di\n",
    "# https://pypi.org/project/tqdm/#pandas-integration\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memuat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('/datasets/imdb_reviews.tsv', sep='\\t', dtype={'votes': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47331 entries, 0 to 47330\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   tconst           47331 non-null  object \n",
      " 1   title_type       47331 non-null  object \n",
      " 2   primary_title    47331 non-null  object \n",
      " 3   original_title   47331 non-null  object \n",
      " 4   start_year       47331 non-null  int64  \n",
      " 5   end_year         47331 non-null  object \n",
      " 6   runtime_minutes  47331 non-null  object \n",
      " 7   is_adult         47331 non-null  int64  \n",
      " 8   genres           47331 non-null  object \n",
      " 9   average_rating   47329 non-null  float64\n",
      " 10  votes            47329 non-null  Int64  \n",
      " 11  review           47331 non-null  object \n",
      " 12  rating           47331 non-null  int64  \n",
      " 13  sp               47331 non-null  object \n",
      " 14  pos              47331 non-null  int64  \n",
      " 15  ds_part          47331 non-null  object \n",
      " 16  idx              47331 non-null  int64  \n",
      "dtypes: Int64(1), float64(1), int64(5), object(10)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan info umum tentang dataset\n",
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari informasi tersebut, dapat dilihat bahwa dataset terdiri dari 47331 baris dan 17 kolom. Beberapa kolom memiliki data yang hilang (non-null count yang lebih rendah dari jumlah baris), seperti average_rating dan votes, yang masing-masing memiliki dua data yang hilang. Hal ini perlu diperhatikan saat melakukan pra-pemrosesan data dan analisis lebih lanjut. Tipe data kolom-kolom telah diidentifikasi, di antaranya terdapat kolom dengan tipe data object, int64, float64, dan Int64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada beberapa kolom yang perlu kita ubah tipe datanya untuk analisis yang lebih baik. Kolom end_year dan runtime_minutes seharusnya memiliki tipe data numerik, bukan objek. Kita juga bisa mengubah kolom is_adult menjadi tipe data boolean untuk mewakili apakah film tersebut untuk dewasa atau tidak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah tipe data kolom end_year menjadi numerik (jika memungkinkan)\n",
    "df_reviews['end_year'] = pd.to_numeric(df_reviews['end_year'], errors='coerce')\n",
    "\n",
    "# Mengubah tipe data kolom runtime_minutes menjadi numerik (jika memungkinkan)\n",
    "df_reviews['runtime_minutes'] = pd.to_numeric(df_reviews['runtime_minutes'], errors='coerce')\n",
    "\n",
    "# Mengubah tipe data kolom is_adult menjadi boolean\n",
    "df_reviews['is_adult'] = df_reviews['is_adult'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>title_type</th>\n",
       "      <th>primary_title</th>\n",
       "      <th>original_title</th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_year</th>\n",
       "      <th>runtime_minutes</th>\n",
       "      <th>is_adult</th>\n",
       "      <th>genres</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>votes</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sp</th>\n",
       "      <th>pos</th>\n",
       "      <th>ds_part</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0068152</td>\n",
       "      <td>movie</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>1971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Comedy,Crime,Drama</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2218</td>\n",
       "      <td>The pakage implies that Warren Beatty and Gold...</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>8335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0068152</td>\n",
       "      <td>movie</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>1971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Comedy,Crime,Drama</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2218</td>\n",
       "      <td>How the hell did they get this made?! Presenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>8336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184</td>\n",
       "      <td>There is no real story the film seems more lik...</td>\n",
       "      <td>3</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>2489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184</td>\n",
       "      <td>Um .... a serious film about troubled teens in...</td>\n",
       "      <td>7</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>9280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0313150</td>\n",
       "      <td>short</td>\n",
       "      <td>'15'</td>\n",
       "      <td>'15'</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Comedy,Drama,Short</td>\n",
       "      <td>6.3</td>\n",
       "      <td>184</td>\n",
       "      <td>I'm totally agree with GarryJohal from Singapo...</td>\n",
       "      <td>9</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>9281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst title_type primary_title original_title  start_year  end_year  \\\n",
       "0  tt0068152      movie             $              $        1971       NaN   \n",
       "1  tt0068152      movie             $              $        1971       NaN   \n",
       "2  tt0313150      short          '15'           '15'        2002       NaN   \n",
       "3  tt0313150      short          '15'           '15'        2002       NaN   \n",
       "4  tt0313150      short          '15'           '15'        2002       NaN   \n",
       "\n",
       "   runtime_minutes  is_adult              genres  average_rating  votes  \\\n",
       "0            121.0     False  Comedy,Crime,Drama             6.3   2218   \n",
       "1            121.0     False  Comedy,Crime,Drama             6.3   2218   \n",
       "2             25.0     False  Comedy,Drama,Short             6.3    184   \n",
       "3             25.0     False  Comedy,Drama,Short             6.3    184   \n",
       "4             25.0     False  Comedy,Drama,Short             6.3    184   \n",
       "\n",
       "                                              review  rating   sp  pos  \\\n",
       "0  The pakage implies that Warren Beatty and Gold...       1  neg    0   \n",
       "1  How the hell did they get this made?! Presenti...       1  neg    0   \n",
       "2  There is no real story the film seems more lik...       3  neg    0   \n",
       "3  Um .... a serious film about troubled teens in...       7  pos    1   \n",
       "4  I'm totally agree with GarryJohal from Singapo...       9  pos    1   \n",
       "\n",
       "  ds_part   idx  \n",
       "0   train  8335  \n",
       "1   train  8336  \n",
       "2    test  2489  \n",
       "3    test  9280  \n",
       "4    test  9281  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan lima baris pertama dari dataset\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tconst: Kode unik IMDb untuk setiap film atau acara TV.\n",
    "2. title_type: Jenis judul (misalnya, film atau acara pendek).\n",
    "3. primary_title: Judul primer film.\n",
    "4. original_title: Judul asli film (jika ada perbedaan antara judul primer dan asli).\n",
    "5. start_year: Tahun rilis film.\n",
    "6. end_year: Tahun akhir rilis film (jika film tersebut merupakan seri dengan akhiran).\n",
    "7. runtime_minutes: Durasi film dalam menit.\n",
    "8. is_adult: Boolean yang menunjukkan apakah film tersebut ditujukan untuk penonton dewasa atau tidak.\n",
    "9. genres: Genre-genre film.\n",
    "10. average_rating: Rating rata-rata IMDb untuk film tersebut.\n",
    "11. votes: Jumlah suara yang diterima oleh film tersebut di IMDb.\n",
    "12. review: Ulasan teks tentang film.\n",
    "13. rating: Nilai rating ulasan (dari 1 hingga 10).\n",
    "14. sp: Sentimen (negatif atau positif) dari ulasan.\n",
    "15. pos: Label positif atau negatif untuk klasifikasi.\n",
    "16. ds_part: Bagian dataset (train atau test).\n",
    "17. idx: Indeks unik untuk setiap entri dalam dataset.\n",
    "\n",
    "Dari lima baris pertama tersebut, kita dapat melihat struktur dataset dan jenis informasi yang disertakan dalam setiap entri. Ini akan membantu kita dalam memahami dataset dan merencanakan langkah-langkah pra-pemrosesan data dan analisis selanjutnya.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_year</th>\n",
       "      <th>runtime_minutes</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>votes</th>\n",
       "      <th>rating</th>\n",
       "      <th>pos</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47331.000000</td>\n",
       "      <td>2279.000000</td>\n",
       "      <td>46843.000000</td>\n",
       "      <td>47329.000000</td>\n",
       "      <td>4.732900e+04</td>\n",
       "      <td>47331.000000</td>\n",
       "      <td>47331.000000</td>\n",
       "      <td>47331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1989.631235</td>\n",
       "      <td>1999.539710</td>\n",
       "      <td>98.759729</td>\n",
       "      <td>5.998278</td>\n",
       "      <td>2.556292e+04</td>\n",
       "      <td>5.484608</td>\n",
       "      <td>0.498954</td>\n",
       "      <td>6279.697999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.600364</td>\n",
       "      <td>11.846141</td>\n",
       "      <td>38.322569</td>\n",
       "      <td>1.494289</td>\n",
       "      <td>8.367004e+04</td>\n",
       "      <td>3.473109</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>3605.702545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1894.000000</td>\n",
       "      <td>1953.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1982.000000</td>\n",
       "      <td>1995.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>8.270000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1998.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>3.197000e+03</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6299.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>1.397400e+04</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9412.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>1.739448e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_year     end_year  runtime_minutes  average_rating  \\\n",
       "count  47331.000000  2279.000000     46843.000000    47329.000000   \n",
       "mean    1989.631235  1999.539710        98.759729        5.998278   \n",
       "std       19.600364    11.846141        38.322569        1.494289   \n",
       "min     1894.000000  1953.000000         1.000000        1.400000   \n",
       "25%     1982.000000  1995.000000        87.000000        5.100000   \n",
       "50%     1998.000000  2004.000000        95.000000        6.300000   \n",
       "75%     2004.000000  2008.000000       109.000000        7.100000   \n",
       "max     2010.000000  2020.000000      1140.000000        9.700000   \n",
       "\n",
       "              votes        rating           pos           idx  \n",
       "count  4.732900e+04  47331.000000  47331.000000  47331.000000  \n",
       "mean   2.556292e+04      5.484608      0.498954   6279.697999  \n",
       "std    8.367004e+04      3.473109      0.500004   3605.702545  \n",
       "min    9.000000e+00      1.000000      0.000000      0.000000  \n",
       "25%    8.270000e+02      2.000000      0.000000   3162.000000  \n",
       "50%    3.197000e+03      4.000000      0.000000   6299.000000  \n",
       "75%    1.397400e+04      9.000000      1.000000   9412.000000  \n",
       "max    1.739448e+06     10.000000      1.000000  12499.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan statistik deskriptif untuk kolom-kolom numerik\n",
    "df_reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start_year dan end_year menunjukkan rentang tahun film/episode dimulai dan berakhir, dengan variasi yang luas.\n",
    "runtime_minutes memperlihatkan variasi durasi film/episode dengan durasi rata-rata sekitar 98.76 menit.\n",
    "average_rating dan rating memberikan informasi tentang rating rata-rata dan jumlah suara, dengan variasi yang signifikan.\n",
    "votes menunjukkan distribusi jumlah suara, yang sangat bervariasi dengan beberapa entri mendapatkan suara yang sangat banyak.\n",
    "pos mungkin adalah kolom biner yang menunjukkan kategori tertentu (misalnya, ulasan positif), dengan sekitar 50% data masuk dalam kategori ini.\n",
    "idx tampaknya merupakan indeks atau penomoran untuk setiap entri dalam dataset, dengan rentang yang luas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23715\n",
       "1    23616\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan distribusi kelas target\n",
    "df_reviews['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil yang ditampilkan adalah distribusi kelas target dari kolom pos. Kolom ini adalah kolom target yang menunjukkan apakah ulasan film dianggap positif (1) atau negatif (0).\n",
    "\n",
    "Dari hasil tersebut, kita dapat melihat bahwa terdapat 23.715 ulasan yang diklasifikasikan sebagai negatif (kelas 0) dan 23.616 ulasan yang diklasifikasikan sebagai positif (kelas 1). Distribusi kelas target ini nampaknya cukup seimbang karena jumlah ulasan positif dan negatif tidak terlalu jauh berbeda satu sama lain. Hal ini penting untuk diperhatikan karena ketidakseimbangan kelas dapat mempengaruhi kinerja model klasifikasi. Dengan distribusi kelas yang seimbang seperti ini, kita dapat melatih model dengan baik tanpa khawatir akan bias kelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tconst                 0\n",
       "title_type             0\n",
       "primary_title          0\n",
       "original_title         0\n",
       "start_year             0\n",
       "end_year           45052\n",
       "runtime_minutes      488\n",
       "is_adult               0\n",
       "genres                 0\n",
       "average_rating         2\n",
       "votes                  2\n",
       "review                 0\n",
       "rating                 0\n",
       "sp                     0\n",
       "pos                    0\n",
       "ds_part                0\n",
       "idx                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan jumlah data yang hilang (missing values) per kolom\n",
    "df_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil yang ditampilkan adalah jumlah data yang hilang (missing values) per kolom dalam dataset IMDb reviews. Setiap nilai non-null dalam DataFrame akan memiliki nilai True, sedangkan nilai yang hilang (null) akan memiliki nilai False. Metode .isnull() digunakan untuk mengidentifikasi nilai yang hilang, kemudian metode .sum() digunakan untuk menghitung jumlah nilai yang hilang (yang direpresentasikan oleh nilai False).\n",
    "\n",
    "Berikut adalah penjelasan untuk setiap kolom yang memiliki data yang hilang:\n",
    "\n",
    "1. end_year: Terdapat 45.052 data yang hilang pada kolom ini. Hal ini menunjukkan bahwa sebagian besar film tidak memiliki tahun akhir rilis, mungkin karena mereka tidak termasuk dalam seri film dengan akhiran.\n",
    "2. runtime_minutes: Terdapat 488 data yang hilang pada kolom ini. Ini menunjukkan bahwa durasi film tidak tersedia untuk sejumlah kecil entri dalam dataset.\n",
    "3. average_rating: Terdapat 2 data yang hilang pada kolom ini. Rating rata-rata IMDb tidak tersedia untuk dua entri dalam dataset.\n",
    "4. votes: Terdapat 2 data yang hilang pada kolom ini. Jumlah suara IMDb tidak tersedia untuk dua entri dalam dataset.\n",
    "\n",
    "Data yang hilang perlu diperhatikan saat melakukan analisis dan pra-pemrosesan data, seperti pengisian data yang hilang atau penghapusan baris yang mengandung data yang hilang, tergantung pada konteksnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita perlu menangani data yang hilang agar tidak mempengaruhi kualitas analisis dan model yang akan kita bangun. Ada beberapa pendekatan yang dapat kita gunakan, seperti mengisi data yang hilang dengan nilai rata-rata atau median untuk kolom numerik, atau mengisi data yang hilang dengan nilai yang paling umum untuk kolom kategorikal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi data yang hilang dengan nilai rata-rata untuk kolom runtime_minutes\n",
    "mean_runtime = df_reviews['runtime_minutes'].mean()\n",
    "df_reviews['runtime_minutes'].fillna(mean_runtime, inplace=True)\n",
    "\n",
    "# Mengisi data yang hilang dengan nilai rata-rata untuk kolom average_rating\n",
    "mean_rating = df_reviews['average_rating'].mean()\n",
    "df_reviews['average_rating'].fillna(mean_rating, inplace=True)\n",
    "\n",
    "# Mengisi data yang hilang untuk kolom end_year dengan nilai -1 (misalnya, untuk menunjukkan bahwa data akhir tahun tidak tersedia)\n",
    "df_reviews['end_year'].fillna(-1, inplace=True)\n",
    "\n",
    "# Mengisi data yang hilang untuk kolom votes dengan nilai 0 (misalnya, untuk menunjukkan bahwa tidak ada suara yang tersedia)\n",
    "df_reviews['votes'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tconst             0\n",
       "title_type         0\n",
       "primary_title      0\n",
       "original_title     0\n",
       "start_year         0\n",
       "end_year           0\n",
       "runtime_minutes    0\n",
       "is_adult           0\n",
       "genres             0\n",
       "average_rating     0\n",
       "votes              0\n",
       "review             0\n",
       "rating             0\n",
       "sp                 0\n",
       "pos                0\n",
       "ds_part            0\n",
       "idx                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan jumlah data yang hilang (missing values) per kolom setelah di lakukan penanganan.\n",
    "df_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus karakter khusus dan tanda baca\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Mengubah teks menjadi huruf kecil\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Menghapus kata-kata yang tidak bermakna (stopwords)\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Tokenisasi teks\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Pra-pemrosesan data untuk kolom review\n",
    "df_reviews['clean_review'] = df_reviews['review'].apply(remove_special_characters)\n",
    "df_reviews['clean_review'] = df_reviews['clean_review'].apply(lowercase_text)\n",
    "df_reviews['clean_review'] = df_reviews['clean_review'].apply(remove_stopwords)\n",
    "df_reviews['tokens'] = df_reviews['clean_review'].apply(tokenize_text)\n",
    "\n",
    "# Menampilkan lima baris pertama dari dataset setelah pra-pemrosesan data\n",
    "df_reviews[['review', 'clean_review', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah penjelasan dari setiap kolom yang ditampilkan:\n",
    "\n",
    "1. Kolom review: Ini adalah ulasan asli dalam bentuk teks sebelum dilakukan pra-pemrosesan data.\n",
    "\n",
    "2. Kolom clean_review: Ini adalah ulasan setelah dilakukan pra-pemrosesan data, termasuk langkah-langkah seperti menghapus karakter khusus, mengubah teks menjadi huruf kecil, dan menghapus kata-kata yang tidak bermakna (stopwords).\n",
    "\n",
    "3. Kolom tokens: Ini adalah token-token (kata-kata) dari ulasan setelah pra-pemrosesan data. Tokenisasi dilakukan setelah membersihkan ulasan, sehingga setiap kata dalam ulasan dipisahkan menjadi token.\n",
    "\n",
    "Dari hasil ini, kita dapat melihat bahwa ulasan telah dibersihkan dari karakter khusus dan tanda baca, dikonversi menjadi huruf kecil, dan kata-kata yang tidak bermakna telah dihapus. Selanjutnya, setiap ulasan telah dipisahkan menjadi token-token yang dapat digunakan untuk pembangunan model klasifikasi. Langkah-langkah ini mempersiapkan data secara efektif untuk analisis dan pembangunan model selanjutnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periksa jumlah film dan ulasan selama beberapa tahun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "ax = axs[0]\n",
    "\n",
    "dft1 = df_reviews[['tconst', 'start_year']].drop_duplicates() \\\n",
    "    ['start_year'].value_counts().sort_index()\n",
    "dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0)\n",
    "dft1.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Jumlah Film Selama Beberapa Tahun')\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "dft2 = df_reviews.groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
    "\n",
    "dft2.plot(kind='bar', stacked=True, label='#ulasan  (neg, pos)', ax=ax)\n",
    "\n",
    "dft2 = df_reviews['start_year'].value_counts().sort_index()\n",
    "dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
    "dft3 = (dft2/dft1).fillna(0)\n",
    "axt = ax.twinx()\n",
    "dft3.reset_index(drop=True).rolling(5).mean().plot(color='orange', label='ulasan per film (rata-rata selama 5 tahun)', ax=axt)\n",
    "\n",
    "lines, labels = axt.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='upper left')\n",
    "\n",
    "ax.set_title('Jumlah Ulasan Selama Beberapa Tahun') \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periksa distribusi jumlah ulasan per film dengan penghitungan yang tepat dan KDE (hanya untuk mengetahui perbedaannya dari penghitungan yang tepat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax = axs[0]\n",
    "dft = df_reviews.groupby('tconst')['review'].count() \\\n",
    "    .value_counts() \\\n",
    "    .sort_index()\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_title('Plot batang #Ulasan Per Film')\n",
    "\n",
    "ax = axs[1]\n",
    "dft = df_reviews.groupby('tconst')['review'].count()\n",
    "sns.kdeplot(dft, ax=ax)\n",
    "ax.set_title('Plot KDE #Ulasan Per Film') \n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axs[0]\n",
    "dft = df_reviews.query('ds_part == \"train\"')['rating'].value_counts().sort_index()\n",
    "dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_ylim([0, 5000])\n",
    "ax.set_title('Train set: distribusi peringkat')\n",
    "\n",
    "ax = axs[1]\n",
    "dft = df_reviews.query('ds_part == \"test\"')['rating'].value_counts().sort_index()\n",
    "dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_ylim([0, 5000])\n",
    "ax.set_title('Train set: distribusi peringkat')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribusi ulasan negatif dan positif selama bertahun-tahun untuk dua bagian *dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1)))\n",
    "\n",
    "ax = axs[0][0]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"train\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft.index = dft.index.astype('int')\n",
    "dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
    "dft.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('*Test set*: jumlah ulasan dari polaritas yang berbeda per tahun')\n",
    "\n",
    "ax = axs[0][1]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"train\"').groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)\n",
    "sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('*Train set*: distribusi dari polaritas yang berbeda per film')\n",
    "\n",
    "ax = axs[1][0]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"test\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft.index = dft.index.astype('int')\n",
    "dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
    "dft.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('*Test set*: jumlah ulasan dari polaritas yang berbeda per tahun')\n",
    "\n",
    "ax = axs[1][1]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"test\"').groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)\n",
    "sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('*Train set*: distribusi dari polaritas yang berbeda per film')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Periksa distribusi kelas target (positif/negatif)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='pos', data=df_reviews)\n",
    "plt.title('Distribusi Kelas Target')\n",
    "plt.xlabel('Kelas')\n",
    "plt.ylabel('Jumlah')\n",
    "plt.show()\n",
    "\n",
    "# Periksa panjang teks ulasan\n",
    "df_reviews['review_length'] = df_reviews['review'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_reviews['review_length'], bins=30, kde=True)\n",
    "plt.title('Distribusi Panjang Teks Ulasan')\n",
    "plt.xlabel('Panjang Teks Ulasan')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.show()\n",
    "\n",
    "# Periksa kata-kata yang paling umum muncul dalam ulasan\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Menggabungkan semua token menjadi satu list\n",
    "all_tokens = list(itertools.chain.from_iterable(df_reviews['tokens'].values))\n",
    "\n",
    "# Menghitung frekuensi kemunculan setiap kata\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "# Mengambil 20 kata yang paling umum\n",
    "common_words = word_freq.most_common(20)\n",
    "\n",
    "# Menampilkan 20 kata yang paling umum\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[0] for word in common_words], y=[word[1] for word in common_words])\n",
    "plt.title('20 Kata yang Paling Umum dalam Ulasan')\n",
    "plt.xlabel('Kata')\n",
    "plt.ylabel('Frekuensi')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosedur Evaluasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyusun evaluasi yang dapat digunakan untuk semua model dalam tugas ini secara rutin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    eval_stats = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "    \n",
    "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
    "        \n",
    "        eval_stats[type] = {}\n",
    "    \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)    \n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "\n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        aps = metrics.average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        if type == 'train':\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'green'\n",
    "\n",
    "        # Scor F1 \n",
    "        ax = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        # menetapkan persilangan untuk beberapa ambang batas\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('threshold')\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'Skor F1') \n",
    "\n",
    "        # ROC\n",
    "        ax = axs[1]    \n",
    "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        # menetapkan persilangan untuk beberapa ambang batas\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')        \n",
    "        ax.set_title(f'Kurva ROC')\n",
    "        \n",
    "        # PRC\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        # menetapkan persilangan untuk beberapa ambang batas\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('recall')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'PRC')        \n",
    "\n",
    "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
    "    \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    \n",
    "    print(df_eval_stats)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita menganggap semua model di bawah menerima teks dalam huruf kecil dan tanpa angka, tanda baca, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['review_norm'] = df_reviews['review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x.lower()))# < masukkan kode di sini >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pemisahan Train / Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untungnya, seluruh *dataset* sudah dibagi menjadi *train/test*. Bendera yang sesuai adalah 'ds_part'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_train = df_reviews.query('ds_part == \"train\"').copy()\n",
    "df_reviews_test = df_reviews.query('ds_part == \"test\"').copy()\n",
    "\n",
    "train_target = df_reviews_train['pos']\n",
    "test_target = df_reviews_test['pos']\n",
    "\n",
    "print(df_reviews_train.shape)\n",
    "print(df_reviews_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menunjukkan bentuk (jumlah baris dan kolom) dari DataFrame df_reviews_train dan df_reviews_test setelah memisahkan data menjadi bagian pelatihan dan pengujian.\n",
    "\n",
    "1. df_reviews_train: DataFrame yang berisi data pelatihan. Setelah pemisahan, terdapat 23.796 baris dan 21 kolom dalam DataFrame ini.\n",
    "\n",
    "2. df_reviews_test: DataFrame yang berisi data pengujian. Setelah pemisahan, terdapat 23.535 baris dan 21 kolom dalam DataFrame ini.\n",
    "\n",
    "Jumlah baris yang berbeda antara data pelatihan dan pengujian menunjukkan bahwa pemisahan data telah dilakukan dengan benar dan bahwa kita memiliki dua set data yang terpisah untuk digunakan dalam pelatihan dan pengujian model. Hal ini penting untuk mencegah kebocoran informasi dan memastikan evaluasi model yang adil dan obyektif.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bekerja dengan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 - Konstan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inisialisasi model\n",
    "model_0 = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Latih model\n",
    "model_0.fit(np.zeros((df_reviews_train.shape[0], 1)), train_target)\n",
    "\n",
    "# Evaluasi model\n",
    "evaluate_model(model_0, np.zeros((df_reviews_train.shape[0], 1)), train_target, np.zeros((df_reviews_test.shape[0], 1)), test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil evaluasi model DummyClassifier dengan strategi 'most_frequent' menunjukkan kinerja model baseline yang sangat sederhana. Berikut adalah penjelasan dari masing-masing metrik yang dihasilkan:\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "- Train: 0.5\n",
    "- Test: 0.5\n",
    "\n",
    "Akurasi adalah proporsi prediksi yang benar. Dalam hal ini, akurasi 0.5 berarti model benar dalam 50% kasus, baik pada data latih (train) maupun data uji (test). Karena DummyClassifier dengan strategi 'most_frequent' selalu memprediksi kelas yang paling sering muncul di data latih, ini menunjukkan bahwa dataset kemungkinan memiliki distribusi kelas yang seimbang (50% positif dan 50% negatif).\n",
    "\n",
    "2. F1 Score:\n",
    "\n",
    "- Train: 0.0\n",
    "- Test: 0.0\n",
    "\n",
    "F1 Score adalah rata-rata harmonis dari precision dan recall. Nilai 0.0 menunjukkan bahwa model tidak memprediksi kelas positif sama sekali, sehingga precision dan recall untuk kelas positif adalah 0. Karena DummyClassifier selalu memprediksi kelas yang paling sering muncul (dalam hal ini mungkin kelas negatif), tidak ada prediksi untuk kelas positif, menghasilkan F1 score 0.0.\n",
    "\n",
    "3. Average Precision Score (APS):\n",
    "\n",
    "- Train: 0.5\n",
    "- Test: 0.5\n",
    "\n",
    "APS adalah metrik yang mengukur area di bawah kurva precision-recall. Nilai 0.5 mengindikasikan performa baseline yang setara dengan tebakan acak. Ini masuk akal karena model ini tidak memberikan prediksi yang bervariasi (hanya memprediksi satu kelas), sehingga precision-recall curve datar pada nilai baseline.\n",
    "\n",
    "4. ROC AUC:\n",
    "\n",
    "- Train: 0.5\n",
    "- Test: 0.5\n",
    "\n",
    "ROC AUC adalah area di bawah kurva ROC (Receiver Operating Characteristic). Nilai 0.5 menunjukkan model tidak lebih baik dari tebakan acak. Ini diharapkan dari DummyClassifier yang selalu memprediksi kelas yang paling sering muncul tanpa mempertimbangkan input fitur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - NLTK, TF-IDF dan LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi dan vektorisasi teks menggunakan TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "train_features_1 = tfidf_vectorizer.fit_transform(df_reviews_train['review_norm'])\n",
    "test_features_1 = tfidf_vectorizer.transform(df_reviews_test['review_norm'])\n",
    "\n",
    "# Inisialisasi dan latih model\n",
    "model_1 = LogisticRegression()\n",
    "model_1.fit(train_features_1, train_target)\n",
    "\n",
    "# Evaluasi model\n",
    "evaluate_model(model_1, train_features_1, train_target, test_features_1, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hasil evaluasi model Logistic Regression yang dilatih menggunakan TF-IDF vektorisasi menunjukkan kinerja yang sangat baik pada dataset yang diberikan. Berikut adalah penjelasan dari masing-masing metrik yang dihasilkan:\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "- Train: 0.94\n",
    "- Test: 0.88\n",
    "\n",
    "Model ini memiliki akurasi yang sangat tinggi pada data latih (94%) dan sedikit lebih rendah pada data uji (88%). Ini menunjukkan bahwa model mampu menangkap pola dalam data dengan baik dan generalisasi yang cukup baik ke data yang tidak terlihat, meskipun ada sedikit penurunan akurasi pada data uji yang bisa menjadi tanda overfitting ringan.\n",
    "\n",
    "2. F1 Score:\n",
    "\n",
    "- Train: 0.94\n",
    "- Test: 0.88\n",
    "\n",
    "Nilai F1 yang tinggi (0.94 pada data latih dan 0.88 pada data uji) menunjukkan bahwa model tidak hanya memiliki akurasi yang baik tetapi juga keseimbangan yang baik antara precision dan recall.\n",
    "\n",
    "3. Average Precision Score (APS):\n",
    "\n",
    "- Train: 0.98\n",
    "- Test: 0.95\n",
    "\n",
    "Nilai APS yang sangat tinggi (0.98 pada data latih dan 0.95 pada data uji) menunjukkan bahwa model memiliki kemampuan yang sangat baik untuk membedakan antara kelas positif dan negatif.\n",
    "\n",
    "4. ROC AUC:\n",
    "\n",
    "- Train: 0.98\n",
    "- Test: 0.95\n",
    "\n",
    "Nilai ROC AUC yang sangat tinggi (0.98 pada data latih dan 0.95 pada data uji) menunjukkan bahwa model memiliki kemampuan yang sangat baik untuk membedakan antara kelas positif dan negatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - spaCy, TF-IDF dan LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Inisialisasi spaCy dengan hanya lemmatizer\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_3(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pra-pemrosesan teks menggunakan spaCy\n",
    "df_reviews_train['review_norm_spacy'] = df_reviews_train['review'].apply(text_preprocessing_3)\n",
    "df_reviews_test['review_norm_spacy'] = df_reviews_test['review'].apply(text_preprocessing_3)\n",
    "\n",
    "# Tokenisasi dan vektorisasi teks menggunakan TF-IDF\n",
    "tfidf_vectorizer_spacy = TfidfVectorizer(stop_words='english')\n",
    "train_features_3 = tfidf_vectorizer_spacy.fit_transform(df_reviews_train['review_norm_spacy'])\n",
    "test_features_3 = tfidf_vectorizer_spacy.transform(df_reviews_test['review_norm_spacy'])\n",
    "\n",
    "# Inisialisasi dan latih model Logistic Regression dengan solver yang lebih cepat\n",
    "model_3 = LogisticRegression(solver='saga', max_iter=1000)\n",
    "model_3.fit(train_features_3, train_target)\n",
    "\n",
    "# Fungsi untuk evaluasi model\n",
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    eval_stats = {}\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
    "        eval_stats[type] = {}\n",
    "    \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [f1_score(target, pred_proba >= threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = roc_curve(target, pred_proba)\n",
    "        roc_auc = roc_auc_score(target, pred_proba)    \n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "\n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(target, pred_proba)\n",
    "        aps = average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        color = 'blue' if type == 'train' else 'green'\n",
    "\n",
    "        # Plot F1 Score\n",
    "        ax = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds - threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])\n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('Threshold')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title('F1 Score')\n",
    "\n",
    "        # Plot ROC Curve\n",
    "        ax = axs[1]\n",
    "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds - threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])\n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title('ROC Curve')\n",
    "\n",
    "        # Plot Precision-Recall Curve\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds - threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])\n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title('Precision-Recall Curve')\n",
    "\n",
    "        eval_stats[type]['Accuracy'] = accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = f1_score(target, pred_target)\n",
    "\n",
    "    df_eval_stats = pd.DataFrame(eval_stats).round(2).reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    print(df_eval_stats)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluasi model\n",
    "evaluate_model(model_3, train_features_3, train_target, test_features_3, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil evaluasi model Logistic Regression dengan pra-pemrosesan teks menggunakan spaCy dan TF-IDF menunjukkan kinerja yang sangat baik. Berikut penjelasan singkatnya:\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "- Train: 0.93\n",
    "- Test: 0.87\n",
    "\n",
    "Model ini memprediksi dengan benar 93% dari data latih dan 87% dari data uji, menunjukkan bahwa model ini sangat akurat.\n",
    "\n",
    "2. F1 Score:\n",
    "\n",
    "- Train: 0.93\n",
    "- Test: 0.87\n",
    "\n",
    "Model ini memiliki keseimbangan yang baik antara precision dan recall pada data latih dan uji, yang berarti model ini mampu menangani kelas positif dan negatif dengan baik.\n",
    "\n",
    "3. Average Precision Score (APS):\n",
    "\n",
    "- Train: 0.98\n",
    "- Test: 0.94\n",
    "\n",
    "Model ini memiliki kemampuan yang sangat baik untuk membedakan antara kelas positif dan negatif, dengan nilai APS yang sangat tinggi, baik pada data latih maupun uji.\n",
    "\n",
    "4. ROC AUC:\n",
    "\n",
    "- Train: 0.98\n",
    "- Test: 0.95\n",
    "\n",
    "Model ini sangat efektif dalam membedakan antara kelas positif dan negatif di berbagai threshold, yang ditunjukkan dengan nilai ROC AUC yang mendekati sempurna.\n",
    "\n",
    "\n",
    "Model Logistic Regression yang dilatih dengan pra-pemrosesan teks menggunakan spaCy dan TF-IDF vektorisasi menunjukkan kinerja yang sangat baik. Akurasi dan F1 score yang tinggi serta nilai APS dan ROC AUC yang hampir sempurna pada data latih dan uji menunjukkan bahwa model ini memiliki kemampuan prediksi yang kuat dan mampu melakukan generalisasi dengan baik meskipun ada sedikit penurunan performa pada data uji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - spaCy, TF-IDF dan LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan latih model\n",
    "model_4 = LGBMClassifier()\n",
    "model_4.fit(train_features_3, train_target)\n",
    "\n",
    "# Evaluasi model\n",
    "evaluate_model(model_4, train_features_3, train_target, test_features_3, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil evaluasi model LGBMClassifier dengan pra-pemrosesan teks menggunakan spaCy dan TF-IDF menunjukkan kinerja yang sangat baik namun sedikit lebih rendah dibandingkan dengan model Logistic Regression sebelumnya. Berikut penjelasan singkatnya:\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "- Train: 0.91\n",
    "- Test: 0.85\n",
    "\n",
    "Model ini memprediksi dengan benar 91% dari data latih dan 85% dari data uji, menunjukkan bahwa model ini sangat akurat, meskipun sedikit lebih rendah daripada model Logistic Regression.\n",
    "\n",
    "2. F1 Score:\n",
    "\n",
    "- Train: 0.91\n",
    "- Test: 0.85\n",
    "\n",
    "Model ini memiliki keseimbangan yang baik antara precision dan recall pada data latih dan uji, yang berarti model ini mampu menangani kelas positif dan negatif dengan baik, meskipun sedikit lebih rendah daripada model Logistic Regression.\n",
    "\n",
    "3. Average Precision Score (APS):\n",
    "\n",
    "- Train: 0.97\n",
    "- Test: 0.93\n",
    "\n",
    "Model ini memiliki kemampuan yang sangat baik untuk membedakan antara kelas positif dan negatif, dengan nilai APS yang sangat tinggi, baik pada data latih maupun uji. Namun, APS sedikit lebih rendah dibandingkan dengan model Logistic Regression.\n",
    "\n",
    "4. ROC AUC:\n",
    "\n",
    "- Train: 0.97\n",
    "- Test: 0.93\n",
    "\n",
    "Model ini sangat efektif dalam membedakan antara kelas positif dan negatif di berbagai threshold, yang ditunjukkan dengan nilai ROC AUC yang sangat tinggi, tetapi sedikit lebih rendah dibandingkan dengan model Logistic Regression.\n",
    "\n",
    "\n",
    "Model LGBMClassifier menunjukkan kinerja yang sangat baik dalam memprediksi data dengan akurasi, F1 score, APS, dan ROC AUC yang tinggi pada data latih dan uji. Meskipun performa model ini sedikit lebih rendah dibandingkan dengan model Logistic Regression, model ini tetap menunjukkan kemampuan prediksi yang kuat dan mampu melakukan generalisasi dengan baik. Performa yang sedikit lebih rendah mungkin menunjukkan bahwa LGBMClassifier sedikit lebih rentan terhadap overfitting atau memerlukan penyesuaian lebih lanjut untuk mencapai performa maksimal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 9 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inisialisasi tokenizer dan model BERT\n",
    "# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BERT_text_to_embeddings(texts, max_length=512, batch_size=100, force_device=None, disable_progress_bar=False):\n",
    "#     ids_list = []\n",
    "#     attention_mask_list = []\n",
    "\n",
    "#     # Tokenisasi teks dan konversi menjadi ID token\n",
    "#     print(\"Tokenizing texts...\")\n",
    "#     for text in texts:\n",
    "#         # Tokenisasi teks\n",
    "#         encoded_text = tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             max_length=max_length,\n",
    "#             add_special_tokens=True,\n",
    "#             padding='max_length',\n",
    "#             return_attention_mask=True,\n",
    "#             truncation=True\n",
    "#         )\n",
    "#         ids_list.append(encoded_text['input_ids'])\n",
    "#         attention_mask_list.append(encoded_text['attention_mask'])\n",
    "\n",
    "#     print(f\"Total texts tokenized: {len(ids_list)}\")\n",
    "    \n",
    "#     # Konversi list ke tensor\n",
    "#     ids_list = torch.tensor(ids_list)\n",
    "#     attention_mask_list = torch.tensor(attention_mask_list)\n",
    "\n",
    "#     # Tentukan perangkat yang akan digunakan\n",
    "#     if force_device is not None:\n",
    "#         device = torch.device(force_device)\n",
    "#     else:\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#     model.to(device)\n",
    "#     print(f\"Using device: {device}\")\n",
    "\n",
    "#     # Dapatkan embedding dalam batch\n",
    "#     embeddings = []\n",
    "#     print(\"Starting batch processing...\")\n",
    "#     for i in tqdm(range(math.ceil(len(ids_list) / batch_size)), disable=disable_progress_bar):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(ids_list))\n",
    "\n",
    "#         ids_batch = ids_list[start_idx:end_idx].to(device)\n",
    "#         attention_mask_batch = attention_mask_list[start_idx:end_idx].to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "#             batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "#         embeddings.append(batch_embeddings[0][:, 0, :].detach().cpu().numpy())\n",
    "#         print(f\"Processed batch {i+1}/{math.ceil(len(ids_list) / batch_size)}\")\n",
    "\n",
    "#     print(\"Batch processing completed.\")\n",
    "#     return np.concatenate(embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Contoh penggunaan dengan CPU\n",
    "# train_features_9 = BERT_text_to_embeddings(df_reviews_train['review_norm'], force_device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cetak bentuk dari hasil embedding untuk memastikan semuanya bekerja dengan benar\n",
    "# print(f\"Shape of BERT embeddings: {train_features_9.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_reviews_train['review_norm'].shape)\n",
    "# print(train_features_9.shape)\n",
    "# print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jika sudah mendapatkan embedding, disarankan untuk menyimpannya agar siap \n",
    "# np.savez_compressed('features_9.npz', train_features_9=train_features_9, test_features_9=test_features_9)\n",
    "\n",
    "# dan muat...\n",
    "# with np.load('features_9.npz') as data:\n",
    "#     train_features_9 = data['train_features_9']\n",
    "#     test_features_9 = data['test_features_9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ulasan Saya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jangan ragu untuk menghapus ulasan ini dan mencoba modelmu sendiri terhadap ulasanmu, ini hanya sekadar contoh saja \n",
    "my_reviews = pd.DataFrame([\n",
    "    'I dont like it that much, not my type of movie.',\n",
    "    'Boring, I even fell asleep in the middle of the movie.',\n",
    "    'The movie is very good, I really like it.',\n",
    "    'Even the actors looked very old and uninterested in the movie. Were they paid to play in this film? Its really low quality.',\n",
    "    'I didnt expect the movie to be this good! The writers really paid attention to every detail.',\n",
    "    'This movie has its strengths and weaknesses, but overall, I feel its a decent film. I might watch it again.',\n",
    "    'Some of the jokes are really not funny. Not a single joke worked, everyone acted annoyingly, even children wouldnt like this!',\n",
    "    'Streaming this movie on Netflix is a bold move & Im very happy to watch episode after episode of this new, interesting, and smart drama.'\n",
    "    \n",
    "], columns=['review'])\n",
    "\n",
    "my_reviews['review_norm'] = my_reviews['review'].apply(text_preprocessing_3)# <masukkan logika normalisasi yang sama di sini sebagaimana pada dataset utama>\n",
    "\n",
    "my_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saya telah mengubah teks ulasan film ke dalam bahasa Inggris dan melakukan normalisasi menggunakan fungsi text_preprocessing_3. Hasil normalisasi ini bertujuan untuk menghilangkan kata-kata yang tidak relevan dan mengubah kata-kata ke bentuk dasarnya (lemma). Berikut adalah penjelasan singkat dari hasilnya:\n",
    "\n",
    "Teks Ulasan Awal vs Teks Normalisasi\n",
    "\n",
    "1. Review Awal: \"I dont like it that much, not my type of movie.\"\n",
    "   \n",
    "   Review Normalisasi: \"not like , type movie .\"\n",
    "\n",
    "- Kata-kata yang tidak relevan seperti \"I\", \"dont\", \"it\", \"that\", \"my\", dan \"of\" dihilangkan. Kata-kata kunci \"not like\" dan \"type movie\" dipertahankan.\n",
    "\n",
    "2. Review Awal: \"Boring, I even fell asleep in the middle of the movie.\"\n",
    "   \n",
    "   Review Normalisasi: \"boring , fall asleep middle movie .\"\n",
    "\n",
    "- Menyederhanakan ulasan dengan menghilangkan kata-kata yang tidak perlu, mempertahankan kata kunci seperti \"boring\", \"fall asleep\", dan \"middle movie\".\n",
    "\n",
    "3. Review Awal: \"The movie is very good, I really like it.\"\n",
    "   \n",
    "   Review Normalisasi: \"movie good , like .\"\n",
    "\n",
    "- Menghilangkan kata-kata penghubung dan mengganti \"really\" dengan bentuk dasarnya.\n",
    "\n",
    "4. Review Awal: \"Even the actors looked very old and uninterested in the movie. Were they paid to play in this film? Its really low quality.\"\n",
    "   \n",
    "   Review Normalisasi: \"actor look old uninterested movie . pay play film .\"\n",
    "\n",
    "- Menyederhanakan ulasan dengan menghilangkan kata-kata tambahan dan mempertahankan makna utama dari ulasan.\n",
    "\n",
    "5. Review Awal: \"I didnt expect the movie to be this good! The writers really paid attention to every detail.\"\n",
    "   \n",
    "   Review Normalisasi: \"not expect movie good ! writer pay attention detail .\"\n",
    "\n",
    "- Menghilangkan kata-kata tidak relevan dan mempertahankan kata kunci yang penting.\n",
    "\n",
    "6. Review Awal: \"This movie has its strengths and weaknesses, but overall, I feel its a decent film. I might watch it again.\"\n",
    "  \n",
    "  Review Normalisasi: \"movie strength weakness , overall , feel decent film .\"\n",
    "\n",
    "- Menghilangkan kata-kata penghubung dan kata keterangan tambahan.\n",
    "\n",
    "7. Review Awal: \"Some of the jokes are really not funny. Not a single joke worked, everyone acted annoyingly, even children wouldnt like this!\"\n",
    "   \n",
    "   Review Normalisasi: \"joke funny . single joke work , act annoyingly , child like !\"\n",
    "\n",
    "- Menghilangkan kata-kata yang tidak relevan dan mempertahankan makna utama dari ulasan.\n",
    "\n",
    "8. Review Awal: \"Streaming this movie on Netflix is a bold move & Im very happy to watch episode after episode of this new, interesting, and smart drama.\"\n",
    "   \n",
    "   Review Normalisasi: \"stream movie Netflix bold & m happy watch episode episode new , interesting , smart drama .\"\n",
    "\n",
    "- Menyederhanakan ulasan dengan menghilangkan kata-kata tambahan dan mempertahankan kata kunci penting.\n",
    "\n",
    "\n",
    "Normalisasi teks menggunakan fungsi text_preprocessing_3 berhasil menyederhanakan ulasan dengan menghilangkan kata-kata yang tidak relevan dan mempertahankan kata-kata kunci yang penting untuk analisis sentimen. Proses ini membantu model machine learning untuk lebih fokus pada informasi penting dalam teks, yang pada akhirnya dapat meningkatkan akurasi prediksi sentimen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_1.predict_proba(tfidf_vectorizer.transform(texts))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saya menggunakan model model_1 untuk memprediksi probabilitas sentimen positif dari teks ulasan film yang telah dinormalisasi. Hasil ini memberikan wawasan mengenai seberapa positif atau negatif model menilai setiap ulasan.\n",
    "\n",
    "Hasil Prediksi Model\n",
    "\n",
    "1. Ulasan: \"not like , type movie .\"\n",
    "- Probabilitas Positif: 0.49\n",
    "- Model menilai ulasan ini netral dengan sedikit kecenderungan ke arah negatif.\n",
    "\n",
    "2. Ulasan: \"boring , fall asleep middle movie .\"\n",
    "- Probabilitas Positif: 0.03\n",
    "- Model menilai ulasan ini sangat negatif, sesuai dengan konteksnya yang menggambarkan kebosanan.\n",
    "\n",
    "3. Ulasan: \"movie good , like .\"\n",
    "- Probabilitas Positif: 0.70\n",
    "- Model menilai ulasan ini positif, sesuai dengan konteksnya yang menggambarkan kesukaan terhadap film.\n",
    "\n",
    "4. Ulasan: \"actor look old uninterested movie . pay play film ? low quality .\"\n",
    "- Probabilitas Positif: 0.23\n",
    "- Model menilai ulasan ini cukup negatif, sesuai dengan konteksnya yang mengkritik kualitas film dan penampilan aktor.\n",
    "\n",
    "5. Ulasan: \"not expect movie good ! writer pay attention detail .\"\n",
    "- Probabilitas Positif: 0.64\n",
    "- Model menilai ulasan ini positif, sesuai dengan konteksnya yang menggambarkan kejutan positif terhadap kualitas film.\n",
    "\n",
    "6. Ulasan: \"movie strength weakness , overall , feel decent film . watch .\"\n",
    "- Probabilitas Positif: 0.70\n",
    "- Model menilai ulasan ini positif, meskipun menyebutkan kelemahan, tetapi secara keseluruhan menunjukkan kepuasan.\n",
    "\n",
    "7. Ulasan: \"joke funny . single joke work , act annoyingly , child not like !\"\n",
    "- Probabilitas Positif: 0.16\n",
    "- Model menilai ulasan ini negatif, sesuai dengan konteksnya yang mengkritik humor dan akting dalam film.\n",
    "\n",
    "8. Ulasan: \"stream movie Netflix bold & m happy watch episode episode new , interesting , smart drama .\"\n",
    "- Probabilitas Positif: 0.79\n",
    "- Model menilai ulasan ini sangat positif, sesuai dengan konteksnya yang menunjukkan kegembiraan dan kepuasan dengan film.\n",
    "\n",
    "\n",
    "Model model_1 memberikan prediksi probabilitas sentimen yang umumnya sesuai dengan konteks ulasan. Ulasan yang mengandung kata-kata positif seperti \"good\", \"like\", dan \"happy\" memiliki probabilitas sentimen positif yang lebih tinggi. Sebaliknya, ulasan dengan kata-kata negatif seperti \"boring\" dan \"uninterested\" memiliki probabilitas sentimen positif yang lebih rendah. Hasil ini menunjukkan bahwa model mampu menangkap nuansa sentimen dari teks yang telah dinormalisasi dengan baik.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_3.predict_proba(tfidf_vectorizer_spacy.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil Prediksi Model\n",
    "\n",
    "1. Ulasan: \"not like , type movie .\"\n",
    "- Probabilitas Positif: 0.49\n",
    "- Model menilai ulasan ini netral dengan sedikit kecenderungan negatif.\n",
    "\n",
    "2. Ulasan: \"boring , fall asleep middle movie .\"\n",
    "- Probabilitas Positif: 0.02\n",
    "- Model menilai ulasan ini sangat negatif, sesuai dengan konteksnya yang menggambarkan kebosanan.\n",
    "\n",
    "3. Ulasan: \"movie good , like .\"\n",
    "- Probabilitas Positif: 0.93\n",
    "- Model menilai ulasan ini sangat positif, sesuai dengan konteksnya yang menggambarkan kesukaan terhadap film.\n",
    "\n",
    "4. Ulasan: \"actor look old uninterested movie . pay play film ? low quality .\"\n",
    "- Probabilitas Positif: 0.13\n",
    "- Model menilai ulasan ini cukup negatif, sesuai dengan konteksnya yang mengkritik kualitas film dan penampilan aktor.\n",
    "\n",
    "5. Ulasan: \"not expect movie good ! writer pay attention detail .\"\n",
    "- Probabilitas Positif: 0.48\n",
    "- Model menilai ulasan ini netral dengan sedikit kecenderungan positif, karena meskipun ada elemen kejutan positif, itu tidak terlalu kuat.\n",
    "\n",
    "6. Ulasan: \"movie strength weakness , overall , feel decent film . watch .\"\n",
    "- Probabilitas Positif: 0.63\n",
    "- Model menilai ulasan ini cukup positif, menunjukkan bahwa meskipun ada kelemahan, ulasan tersebut secara keseluruhan masih positif.\n",
    "\n",
    "7. Ulasan: \"joke funny . single joke work , act annoyingly , child not like !\"\n",
    "- Probabilitas Positif: 0.16\n",
    "- Model menilai ulasan ini cukup negatif, sesuai dengan konteksnya yang mengkritik humor dan akting dalam film.\n",
    "\n",
    "8. Ulasan: \"stream movie Netflix bold & m happy watch episode episode new , interesting , smart drama .\"\n",
    "- Probabilitas Positif: 0.82\n",
    "- Model menilai ulasan ini sangat positif, sesuai dengan konteksnya yang menunjukkan kegembiraan dan kepuasan dengan film.\n",
    "\n",
    "\n",
    "Model model_3 menunjukkan kemampuan yang baik dalam menilai sentimen ulasan film. Hasil prediksi probabilitas sentimen positif umumnya sesuai dengan konteks ulasan:\n",
    "- Positif Tinggi (0.80-0.93): Ulasan yang mengandung kata-kata positif dan ekspresi kesukaan terhadap film.\n",
    "- Netral hingga Positif Moderat (0.48-0.63): Ulasan yang menunjukkan beberapa kekurangan tetapi tetap memberikan penilaian positif secara keseluruhan.\n",
    "- Negatif (0.02-0.16): Ulasan yang mengandung kritik kuat atau menyatakan ketidaksukaan terhadap film.\n",
    "\n",
    "Secara keseluruhan, model ini efektif dalam mengklasifikasikan sentimen ulasan film berdasarkan teks yang dinormalisasi.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "tfidf_vectorizer_4 = tfidf_vectorizer_spacy\n",
    "my_reviews_pred_prob = model_4.predict_proba(tfidf_vectorizer_4.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil Prediksi Model\n",
    "\n",
    "1. Ulasan: \"not like , type movie .\"\n",
    "- Probabilitas Positif: 0.58\n",
    "- Model menilai ulasan ini cenderung netral dengan sedikit kecenderungan positif.\n",
    "\n",
    "2. Ulasan: \"boring , fall asleep middle movie .\"\n",
    "- Probabilitas Positif: 0.12\n",
    "- Model menilai ulasan ini sangat negatif, sesuai dengan konteksnya yang menggambarkan kebosanan.\n",
    "\n",
    "3. Ulasan: \"movie good , like .\"\n",
    "- Probabilitas Positif: 0.81\n",
    "- Model menilai ulasan ini sangat positif, sesuai dengan konteksnya yang menggambarkan kesukaan terhadap film.\n",
    "\n",
    "4. Ulasan: \"actor look old uninterested movie . pay play film ? low quality .\"\n",
    "- Probabilitas Positif: 0.39\n",
    "- Model menilai ulasan ini cukup negatif, sesuai dengan konteksnya yang mengkritik kualitas film dan penampilan aktor.\n",
    "\n",
    "5. Ulasan: \"not expect movie good ! writer pay attention detail .\"\n",
    "- Probabilitas Positif: 0.73\n",
    "- Model menilai ulasan ini cukup positif, menunjukkan apresiasi terhadap perhatian detail dalam film.\n",
    "\n",
    "7. Ulasan: \"movie strength weakness , overall , feel decent film . watch .\"\n",
    "- Probabilitas Positif: 0.55\n",
    "- Model menilai ulasan ini netral dengan sedikit kecenderungan positif, mencerminkan penilaian campuran.\n",
    "\n",
    "8. Ulasan: \"joke funny . single joke work , act annoyingly , child not like !\"\n",
    "- Probabilitas Positif: 0.55\n",
    "- Model menilai ulasan ini netral dengan sedikit kecenderungan positif, meskipun kritik terhadap humor dalam film.\n",
    "\n",
    "9. Ulasan: \"stream movie Netflix bold & m happy watch episode episode new , interesting , smart drama .\"\n",
    "- Probabilitas Positif: 0.65\n",
    "- Model menilai ulasan ini positif, sesuai dengan konteksnya yang menunjukkan kegembiraan dan kepuasan dengan film.\n",
    "\n",
    "\n",
    "Model model_4 menunjukkan kemampuan yang baik dalam menilai sentimen ulasan film. Hasil prediksi probabilitas sentimen positif umumnya sesuai dengan konteks ulasan.\n",
    "Secara keseluruhan, model ini efektif dalam mengklasifikasikan sentimen ulasan film berdasarkan teks yang dinormalisasi.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = my_reviews['review_norm']\n",
    "\n",
    "# my_reviews_features_9 = BERT_text_to_embeddings(texts, disable_progress_bar=True)\n",
    "\n",
    "# my_reviews_pred_prob = model_9.predict_proba(my_reviews_features_9)[:, 1]\n",
    "\n",
    "# for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "#     print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proyek ini bertujuan untuk membangun model machine learning yang dapat mengklasifikasikan sentimen dari ulasan film. Kita ingin mengetahui apakah suatu ulasan bersentimen positif atau negatif berdasarkan teks yang diberikan.\n",
    "\n",
    "Langkah-Langkah yang Dilakukan\n",
    "\n",
    "1. Pengumpulan dan Prapemrosesan Data:\n",
    "- Kami mulai dengan mengumpulkan dataset ulasan film.\n",
    "- Teks ulasan dinormalisasi menggunakan berbagai teknik seperti tokenisasi, lemmatization, dan penghapusan kata-kata berhenti (stop words) menggunakan pustaka spaCy.\n",
    "\n",
    "2. Vektorisasi Teks:\n",
    "- Teks yang telah dinormalisasi diubah menjadi representasi numerik menggunakan TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "- Dua vektorisasi berbeda digunakan: satu dengan tokenizer standar dan satu dengan tokenizer spaCy.\n",
    "\n",
    "3. Pelatihan Model:\n",
    "- Beberapa model machine learning diuji, termasuk Logistic Regression dengan solver 'saga' yang lebih cepat.\n",
    "- Model dilatih menggunakan fitur yang dihasilkan dari vektorisasi TF-IDF dan target variabel (label sentimen).\n",
    "\n",
    "4. Evaluasi Model: \n",
    "- Model dievaluasi menggunakan metrik seperti Accuracy, F1 Score, ROC AUC, dan Average Precision Score (APS).\n",
    "- Hasil evaluasi divisualisasikan dengan plotting kurva F1 Score, ROC Curve, dan Precision-Recall Curve untuk membandingkan performa model pada data pelatihan dan pengujian.\n",
    "\n",
    "5. Prediksi pada Ulasan:\n",
    "- Model yang telah dilatih digunakan untuk memprediksi probabilitas sentimen dari ulasan baru yang diberikan dalam bahasa Inggris.\n",
    "- Ulasan ini juga dinormalisasi dan diubah menjadi fitur menggunakan teknik yang sama sebelum digunakan untuk prediksi oleh model.\n",
    "\n",
    "Hasil yang Didapatkan\n",
    "1. Model Sentimen:\n",
    "- Model berhasil membedakan antara ulasan positif dan negatif dengan tingkat akurasi yang baik.\n",
    "- Model mampu mengidentifikasi nuansa dalam ulasan, seperti kritik dan pujian, dengan cukup tepat.\n",
    "2. Prediksi Ulasan:\n",
    "- Prediksi probabilitas sentimen menunjukkan bahwa model mampu memberikan nilai yang masuk akal sesuai dengan konteks ulasan.\n",
    "- Ulasan yang jelas positif mendapatkan skor probabilitas tinggi, sementara ulasan yang negatif mendapatkan skor rendah.\n",
    "\n",
    "Proyek ini berhasil membangun model machine learning yang efektif untuk mengklasifikasikan sentimen ulasan film. Model ini menggunakan pendekatan prapemrosesan teks yang komprehensif dan vektorisasi TF-IDF untuk menghasilkan fitur. Evaluasi dan hasil prediksi menunjukkan bahwa model dapat diandalkan untuk mengidentifikasi sentimen positif dan negatif dari ulasan film yang diberikan. Proyek ini memberikan dasar yang solid untuk pengembangan lebih lanjut dalam analisis sentimen dan aplikasi di bidang ulasan produk atau konten lainnya.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daftar Periksa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  *Notebook* dibuka \n",
    "- [x]  Data teks telah dimuat dan dilakukan pra-pemrosesan untuk vektorisasi \n",
    "- [x]  Data teks telah diubah menjadi vektor \n",
    "- [x]  Model telah terlatih dan diuji \n",
    "- [x]  Ambang batas metrik tercapai \n",
    "- [x]  Semua kode sel tersusun sesuai urutan eksekusinya \n",
    "- [x]  Semua kode sel bisa dieksekusi tanpa *error* \n",
    "- [x]  Terdapat kesimpulan "
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 6,
    "start_time": "2024-05-16T12:21:27.074Z"
   },
   {
    "duration": 1819,
    "start_time": "2024-05-16T12:21:53.970Z"
   },
   {
    "duration": 16,
    "start_time": "2024-05-16T12:21:56.805Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-16T12:21:56.994Z"
   },
   {
    "duration": 1076,
    "start_time": "2024-05-16T12:21:57.327Z"
   },
   {
    "duration": 50,
    "start_time": "2024-05-16T12:21:58.406Z"
   },
   {
    "duration": 114,
    "start_time": "2024-05-16T12:21:58.459Z"
   },
   {
    "duration": 36,
    "start_time": "2024-05-16T12:21:58.576Z"
   },
   {
    "duration": 75,
    "start_time": "2024-05-16T12:21:58.615Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-16T12:21:58.715Z"
   },
   {
    "duration": 36,
    "start_time": "2024-05-16T12:21:58.908Z"
   },
   {
    "duration": 12,
    "start_time": "2024-05-16T12:21:59.356Z"
   },
   {
    "duration": 28,
    "start_time": "2024-05-16T12:21:59.513Z"
   },
   {
    "duration": 109037,
    "start_time": "2024-05-16T12:21:59.677Z"
   },
   {
    "duration": 6135,
    "start_time": "2024-05-16T12:23:48.718Z"
   },
   {
    "duration": 1437,
    "start_time": "2024-05-16T12:23:54.856Z"
   },
   {
    "duration": 17,
    "start_time": "2024-05-16T12:23:56.297Z"
   },
   {
    "duration": 711,
    "start_time": "2024-05-16T12:23:56.317Z"
   },
   {
    "duration": 7253,
    "start_time": "2024-05-16T12:23:57.030Z"
   },
   {
    "duration": 3807,
    "start_time": "2024-05-16T12:24:04.286Z"
   },
   {
    "duration": 31,
    "start_time": "2024-05-16T12:24:08.096Z"
   },
   {
    "duration": 1480,
    "start_time": "2024-05-16T12:24:08.130Z"
   },
   {
    "duration": 69,
    "start_time": "2024-05-16T12:24:09.617Z"
   },
   {
    "duration": 25,
    "start_time": "2024-05-16T12:24:09.689Z"
   },
   {
    "duration": 1428,
    "start_time": "2024-05-16T12:24:09.717Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T12:24:11.148Z"
   },
   {
    "duration": 17584,
    "start_time": "2024-05-16T12:24:11.156Z"
   },
   {
    "duration": 1813,
    "start_time": "2024-05-16T12:24:28.744Z"
   },
   {
    "duration": 4355,
    "start_time": "2024-05-16T12:24:30.560Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T12:24:34.920Z"
   },
   {
    "duration": 1183307,
    "start_time": "2024-05-16T12:24:34.929Z"
   },
   {
    "duration": 47,
    "start_time": "2024-05-16T12:44:18.239Z"
   },
   {
    "duration": 53069,
    "start_time": "2024-05-16T12:44:18.288Z"
   },
   {
    "duration": 296,
    "start_time": "2024-05-16T12:45:11.360Z"
   },
   {
    "duration": 2542,
    "start_time": "2024-05-16T12:45:11.659Z"
   },
   {
    "duration": 12,
    "start_time": "2024-05-16T12:45:14.212Z"
   },
   {
    "duration": 962,
    "start_time": "2024-05-16T13:39:57.070Z"
   },
   {
    "duration": 1813,
    "start_time": "2024-05-16T13:40:10.782Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T13:40:13.768Z"
   },
   {
    "duration": 1743,
    "start_time": "2024-05-16T13:40:21.424Z"
   },
   {
    "duration": 15,
    "start_time": "2024-05-16T13:40:25.290Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-16T13:40:26.527Z"
   },
   {
    "duration": 1045,
    "start_time": "2024-05-16T13:40:28.253Z"
   },
   {
    "duration": 39,
    "start_time": "2024-05-16T13:40:30.166Z"
   },
   {
    "duration": 84,
    "start_time": "2024-05-16T13:40:33.813Z"
   },
   {
    "duration": 15,
    "start_time": "2024-05-16T13:40:35.034Z"
   },
   {
    "duration": 78,
    "start_time": "2024-05-16T13:40:35.769Z"
   },
   {
    "duration": 6,
    "start_time": "2024-05-16T13:40:37.172Z"
   },
   {
    "duration": 26,
    "start_time": "2024-05-16T13:40:38.527Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-16T13:40:39.982Z"
   },
   {
    "duration": 28,
    "start_time": "2024-05-16T13:40:40.475Z"
   },
   {
    "duration": 99735,
    "start_time": "2024-05-16T13:40:40.907Z"
   },
   {
    "duration": 5944,
    "start_time": "2024-05-16T13:42:20.645Z"
   },
   {
    "duration": 1401,
    "start_time": "2024-05-16T13:42:26.592Z"
   },
   {
    "duration": 18,
    "start_time": "2024-05-16T13:42:27.996Z"
   },
   {
    "duration": 701,
    "start_time": "2024-05-16T13:42:28.016Z"
   },
   {
    "duration": 7313,
    "start_time": "2024-05-16T13:42:28.721Z"
   },
   {
    "duration": 3794,
    "start_time": "2024-05-16T13:42:36.038Z"
   },
   {
    "duration": 22,
    "start_time": "2024-05-16T13:42:39.836Z"
   },
   {
    "duration": 1491,
    "start_time": "2024-05-16T13:42:39.861Z"
   },
   {
    "duration": 80,
    "start_time": "2024-05-16T13:42:41.357Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T13:42:41.440Z"
   },
   {
    "duration": 1413,
    "start_time": "2024-05-16T13:42:41.448Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T13:42:42.864Z"
   },
   {
    "duration": 18098,
    "start_time": "2024-05-16T13:42:42.872Z"
   },
   {
    "duration": 16098,
    "start_time": "2024-05-16T13:44:11.893Z"
   },
   {
    "duration": 1572,
    "start_time": "2024-05-16T13:44:27.996Z"
   },
   {
    "duration": 4695,
    "start_time": "2024-05-16T13:44:29.571Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T13:44:34.270Z"
   },
   {
    "duration": 1150914,
    "start_time": "2024-05-16T14:05:37.233Z"
   },
   {
    "duration": 6,
    "start_time": "2024-05-16T14:24:48.151Z"
   },
   {
    "duration": 54409,
    "start_time": "2024-05-16T14:24:48.160Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-16T14:25:42.572Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-16T14:25:42.580Z"
   },
   {
    "duration": 30,
    "start_time": "2024-05-16T14:25:42.586Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T14:25:42.619Z"
   },
   {
    "duration": 861,
    "start_time": "2024-05-16T14:25:42.627Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.491Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.493Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.495Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.513Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.515Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.519Z"
   },
   {
    "duration": 0,
    "start_time": "2024-05-16T14:25:43.521Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-16T14:29:44.589Z"
   },
   {
    "duration": 34,
    "start_time": "2024-05-16T14:29:47.667Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-16T14:30:02.844Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-16T14:30:41.785Z"
   },
   {
    "duration": 11,
    "start_time": "2024-05-16T14:30:43.094Z"
   },
   {
    "duration": 55,
    "start_time": "2024-05-16T14:33:52.709Z"
   },
   {
    "duration": 2938,
    "start_time": "2024-05-16T14:34:20.022Z"
   },
   {
    "duration": 60,
    "start_time": "2024-05-16T14:35:01.198Z"
   },
   {
    "duration": 41,
    "start_time": "2024-05-16T14:38:36.425Z"
   },
   {
    "duration": 36,
    "start_time": "2024-05-16T14:42:10.531Z"
   },
   {
    "duration": 37,
    "start_time": "2024-05-16T14:42:17.538Z"
   },
   {
    "duration": 10,
    "start_time": "2024-05-16T14:45:59.147Z"
   },
   {
    "duration": 40,
    "start_time": "2024-05-16T14:46:23.384Z"
   },
   {
    "duration": 252,
    "start_time": "2024-05-16T14:47:23.115Z"
   },
   {
    "duration": 103,
    "start_time": "2024-05-16T14:51:54.998Z"
   },
   {
    "duration": 35,
    "start_time": "2024-05-16T14:54:47.218Z"
   },
   {
    "duration": 96,
    "start_time": "2024-05-16T14:56:18.824Z"
   },
   {
    "duration": 103,
    "start_time": "2024-05-16T14:57:00.112Z"
   },
   {
    "duration": 98,
    "start_time": "2024-05-16T14:59:16.363Z"
   },
   {
    "duration": 105,
    "start_time": "2024-05-16T15:01:14.907Z"
   },
   {
    "duration": 116,
    "start_time": "2024-05-16T15:01:52.849Z"
   },
   {
    "duration": 58,
    "start_time": "2024-05-16T15:04:16.234Z"
   },
   {
    "duration": 38,
    "start_time": "2024-05-16T15:05:04.388Z"
   },
   {
    "duration": 41,
    "start_time": "2024-05-16T15:05:19.701Z"
   },
   {
    "duration": 99,
    "start_time": "2024-05-16T15:08:11.689Z"
   },
   {
    "duration": 179,
    "start_time": "2024-05-16T15:09:07.529Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-16T15:10:17.215Z"
   },
   {
    "duration": 69,
    "start_time": "2024-05-16T15:17:37.676Z"
   },
   {
    "duration": 74,
    "start_time": "2024-05-16T15:19:25.838Z"
   },
   {
    "duration": 114,
    "start_time": "2024-05-16T15:19:55.000Z"
   },
   {
    "duration": 1832,
    "start_time": "2024-05-16T15:20:08.371Z"
   },
   {
    "duration": 17,
    "start_time": "2024-05-16T15:20:15.274Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-16T15:20:17.616Z"
   },
   {
    "duration": 1027,
    "start_time": "2024-05-16T15:20:18.837Z"
   },
   {
    "duration": 55,
    "start_time": "2024-05-16T15:20:19.867Z"
   },
   {
    "duration": 102,
    "start_time": "2024-05-16T15:20:19.925Z"
   },
   {
    "duration": 16,
    "start_time": "2024-05-16T15:20:20.031Z"
   },
   {
    "duration": 96,
    "start_time": "2024-05-16T15:20:20.050Z"
   },
   {
    "duration": 7,
    "start_time": "2024-05-16T15:20:20.338Z"
   },
   {
    "duration": 38,
    "start_time": "2024-05-16T15:20:20.687Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-16T15:20:21.162Z"
   },
   {
    "duration": 40,
    "start_time": "2024-05-16T15:20:21.327Z"
   },
   {
    "duration": 6040,
    "start_time": "2024-05-17T07:50:44.706Z"
   },
   {
    "duration": 16,
    "start_time": "2024-05-17T07:50:52.031Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-17T07:50:52.300Z"
   },
   {
    "duration": 1202,
    "start_time": "2024-05-17T07:50:52.667Z"
   },
   {
    "duration": 51,
    "start_time": "2024-05-17T07:50:53.873Z"
   },
   {
    "duration": 102,
    "start_time": "2024-05-17T07:50:53.928Z"
   },
   {
    "duration": 19,
    "start_time": "2024-05-17T07:50:54.035Z"
   },
   {
    "duration": 98,
    "start_time": "2024-05-17T07:50:54.058Z"
   },
   {
    "duration": 8,
    "start_time": "2024-05-17T07:50:54.161Z"
   },
   {
    "duration": 30,
    "start_time": "2024-05-17T07:50:54.353Z"
   },
   {
    "duration": 28,
    "start_time": "2024-05-17T07:50:54.796Z"
   },
   {
    "duration": 73,
    "start_time": "2024-05-17T07:50:54.948Z"
   },
   {
    "duration": 108232,
    "start_time": "2024-05-17T07:50:55.085Z"
   },
   {
    "duration": 6149,
    "start_time": "2024-05-17T07:52:43.321Z"
   },
   {
    "duration": 1459,
    "start_time": "2024-05-17T07:52:49.473Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-17T07:52:50.939Z"
   },
   {
    "duration": 756,
    "start_time": "2024-05-17T07:52:50.952Z"
   },
   {
    "duration": 7864,
    "start_time": "2024-05-17T07:52:51.713Z"
   },
   {
    "duration": 4092,
    "start_time": "2024-05-17T07:52:59.580Z"
   },
   {
    "duration": 39,
    "start_time": "2024-05-17T07:53:03.676Z"
   },
   {
    "duration": 1651,
    "start_time": "2024-05-17T07:53:03.718Z"
   },
   {
    "duration": 85,
    "start_time": "2024-05-17T07:53:05.376Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-17T07:53:05.464Z"
   },
   {
    "duration": 1478,
    "start_time": "2024-05-17T07:53:05.474Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-17T07:53:06.955Z"
   },
   {
    "duration": 20280,
    "start_time": "2024-05-17T07:53:06.964Z"
   },
   {
    "duration": 10885,
    "start_time": "2024-05-17T07:53:27.248Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-17T07:53:38.136Z"
   },
   {
    "duration": 1255948,
    "start_time": "2024-05-17T07:53:38.148Z"
   },
   {
    "duration": 46,
    "start_time": "2024-05-17T08:14:34.098Z"
   },
   {
    "duration": 55556,
    "start_time": "2024-05-17T08:14:34.146Z"
   },
   {
    "duration": 338,
    "start_time": "2024-05-17T08:15:29.711Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-17T08:15:30.052Z"
   },
   {
    "duration": 6,
    "start_time": "2024-05-17T08:15:30.060Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-17T08:15:30.069Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-17T08:15:30.077Z"
   },
   {
    "duration": 23,
    "start_time": "2024-05-17T08:15:30.084Z"
   },
   {
    "duration": 7,
    "start_time": "2024-05-17T08:15:30.112Z"
   },
   {
    "duration": 91,
    "start_time": "2024-05-17T08:15:30.123Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-17T08:15:30.221Z"
   },
   {
    "duration": 46,
    "start_time": "2024-05-17T08:15:30.233Z"
   },
   {
    "duration": 46,
    "start_time": "2024-05-17T08:15:30.310Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-17T08:15:30.359Z"
   },
   {
    "duration": 6671,
    "start_time": "2024-05-18T15:38:08.251Z"
   },
   {
    "duration": 16,
    "start_time": "2024-05-18T15:38:14.925Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T15:38:14.944Z"
   },
   {
    "duration": 1031,
    "start_time": "2024-05-18T15:38:24.065Z"
   },
   {
    "duration": 47,
    "start_time": "2024-05-18T15:38:25.099Z"
   },
   {
    "duration": 88,
    "start_time": "2024-05-18T15:38:26.430Z"
   },
   {
    "duration": 25,
    "start_time": "2024-05-18T15:38:26.739Z"
   },
   {
    "duration": 74,
    "start_time": "2024-05-18T15:38:29.200Z"
   },
   {
    "duration": 8,
    "start_time": "2024-05-18T15:38:30.609Z"
   },
   {
    "duration": 29,
    "start_time": "2024-05-18T15:38:31.861Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-18T15:38:33.640Z"
   },
   {
    "duration": 28,
    "start_time": "2024-05-18T15:38:34.104Z"
   },
   {
    "duration": 100497,
    "start_time": "2024-05-18T15:38:35.359Z"
   },
   {
    "duration": 5975,
    "start_time": "2024-05-18T15:40:15.860Z"
   },
   {
    "duration": 1501,
    "start_time": "2024-05-18T15:40:21.839Z"
   },
   {
    "duration": 10,
    "start_time": "2024-05-18T15:40:23.345Z"
   },
   {
    "duration": 964,
    "start_time": "2024-05-18T15:40:23.358Z"
   },
   {
    "duration": 7500,
    "start_time": "2024-05-18T15:40:24.325Z"
   },
   {
    "duration": 3955,
    "start_time": "2024-05-18T15:40:31.831Z"
   },
   {
    "duration": 25,
    "start_time": "2024-05-18T15:40:35.790Z"
   },
   {
    "duration": 1583,
    "start_time": "2024-05-18T15:40:35.833Z"
   },
   {
    "duration": 68,
    "start_time": "2024-05-18T15:40:37.431Z"
   },
   {
    "duration": 27,
    "start_time": "2024-05-18T15:40:37.503Z"
   },
   {
    "duration": 1626,
    "start_time": "2024-05-18T15:40:37.533Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T15:40:39.163Z"
   },
   {
    "duration": 19832,
    "start_time": "2024-05-18T15:40:39.171Z"
   },
   {
    "duration": 13091,
    "start_time": "2024-05-18T15:40:59.006Z"
   },
   {
    "duration": 6,
    "start_time": "2024-05-18T15:41:12.101Z"
   },
   {
    "duration": 1274598,
    "start_time": "2024-05-18T15:41:12.111Z"
   },
   {
    "duration": 167,
    "start_time": "2024-05-18T16:02:26.714Z"
   },
   {
    "duration": 55995,
    "start_time": "2024-05-18T16:02:26.884Z"
   },
   {
    "duration": 503,
    "start_time": "2024-05-18T16:03:22.883Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T16:03:23.389Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T16:03:23.397Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-18T16:03:23.404Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T16:03:23.416Z"
   },
   {
    "duration": 6,
    "start_time": "2024-05-18T16:03:23.424Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T16:03:23.433Z"
   },
   {
    "duration": 54,
    "start_time": "2024-05-18T16:03:23.441Z"
   },
   {
    "duration": 32,
    "start_time": "2024-05-18T16:03:23.502Z"
   },
   {
    "duration": 39,
    "start_time": "2024-05-18T16:03:23.538Z"
   },
   {
    "duration": 67,
    "start_time": "2024-05-18T16:03:23.582Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-18T16:03:23.652Z"
   },
   {
    "duration": 5253,
    "start_time": "2024-05-18T16:40:54.661Z"
   },
   {
    "duration": 21,
    "start_time": "2024-05-18T16:40:59.918Z"
   },
   {
    "duration": 6,
    "start_time": "2024-05-18T16:41:01.071Z"
   },
   {
    "duration": 1214,
    "start_time": "2024-05-18T16:41:04.864Z"
   },
   {
    "duration": 115,
    "start_time": "2024-05-18T16:41:22.868Z"
   },
   {
    "duration": 1996,
    "start_time": "2024-05-18T16:44:00.312Z"
   },
   {
    "duration": 16,
    "start_time": "2024-05-18T16:44:04.954Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-18T16:44:05.979Z"
   },
   {
    "duration": 951,
    "start_time": "2024-05-18T16:44:06.857Z"
   },
   {
    "duration": 44,
    "start_time": "2024-05-18T16:44:07.811Z"
   },
   {
    "duration": 112,
    "start_time": "2024-05-18T16:44:07.858Z"
   },
   {
    "duration": 30,
    "start_time": "2024-05-18T16:44:07.974Z"
   },
   {
    "duration": 86,
    "start_time": "2024-05-18T16:44:08.472Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-18T16:44:09.506Z"
   },
   {
    "duration": 36,
    "start_time": "2024-05-18T16:44:10.213Z"
   },
   {
    "duration": 8,
    "start_time": "2024-05-18T16:44:11.082Z"
   },
   {
    "duration": 33,
    "start_time": "2024-05-18T16:44:11.247Z"
   },
   {
    "duration": 102818,
    "start_time": "2024-05-18T16:44:11.738Z"
   },
   {
    "duration": 6091,
    "start_time": "2024-05-18T16:45:54.560Z"
   },
   {
    "duration": 1508,
    "start_time": "2024-05-18T16:46:00.655Z"
   },
   {
    "duration": 8,
    "start_time": "2024-05-18T16:46:02.167Z"
   },
   {
    "duration": 779,
    "start_time": "2024-05-18T16:46:02.178Z"
   },
   {
    "duration": 7573,
    "start_time": "2024-05-18T16:46:02.960Z"
   },
   {
    "duration": 3971,
    "start_time": "2024-05-18T16:46:10.537Z"
   },
   {
    "duration": 36,
    "start_time": "2024-05-18T16:46:14.512Z"
   },
   {
    "duration": 1498,
    "start_time": "2024-05-18T16:46:14.551Z"
   },
   {
    "duration": 77,
    "start_time": "2024-05-18T16:46:16.055Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T16:46:16.135Z"
   },
   {
    "duration": 1489,
    "start_time": "2024-05-18T16:46:16.146Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T16:46:17.639Z"
   },
   {
    "duration": 17619,
    "start_time": "2024-05-18T16:46:17.646Z"
   },
   {
    "duration": 11924,
    "start_time": "2024-05-18T16:46:35.269Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T16:46:47.196Z"
   },
   {
    "duration": 1195979,
    "start_time": "2024-05-18T16:46:47.204Z"
   },
   {
    "duration": 82,
    "start_time": "2024-05-18T17:06:43.186Z"
   },
   {
    "duration": 55304,
    "start_time": "2024-05-18T17:06:43.271Z"
   },
   {
    "duration": 421,
    "start_time": "2024-05-18T17:07:38.578Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T17:07:39.002Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T17:07:39.009Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T17:07:39.018Z"
   },
   {
    "duration": 3,
    "start_time": "2024-05-18T17:07:39.025Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T17:07:39.032Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T17:07:39.038Z"
   },
   {
    "duration": 53,
    "start_time": "2024-05-18T17:07:39.046Z"
   },
   {
    "duration": 10,
    "start_time": "2024-05-18T17:07:39.135Z"
   },
   {
    "duration": 38,
    "start_time": "2024-05-18T17:07:39.148Z"
   },
   {
    "duration": 82,
    "start_time": "2024-05-18T17:07:39.189Z"
   },
   {
    "duration": 5,
    "start_time": "2024-05-18T17:07:39.275Z"
   },
   {
    "duration": 1930,
    "start_time": "2024-05-18T18:23:40.110Z"
   },
   {
    "duration": 17,
    "start_time": "2024-05-18T18:23:42.044Z"
   },
   {
    "duration": 4,
    "start_time": "2024-05-18T18:23:42.064Z"
   },
   {
    "duration": 1038,
    "start_time": "2024-05-18T18:23:42.073Z"
   },
   {
    "duration": 40,
    "start_time": "2024-05-18T18:23:43.113Z"
   },
   {
    "duration": 102,
    "start_time": "2024-05-18T18:23:43.155Z"
   },
   {
    "duration": 46,
    "start_time": "2024-05-18T18:23:43.260Z"
   },
   {
    "duration": 72,
    "start_time": "2024-05-18T18:23:43.309Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-18T18:23:43.400Z"
   },
   {
    "duration": 29,
    "start_time": "2024-05-18T18:23:43.414Z"
   },
   {
    "duration": 9,
    "start_time": "2024-05-18T18:23:43.446Z"
   },
   {
    "duration": 68,
    "start_time": "2024-05-18T18:23:43.457Z"
   },
   {
    "duration": 6175,
    "start_time": "2024-07-11T14:03:13.692Z"
   },
   {
    "duration": 16,
    "start_time": "2024-07-11T14:03:19.872Z"
   },
   {
    "duration": 4,
    "start_time": "2024-07-11T14:03:19.891Z"
   },
   {
    "duration": 1174,
    "start_time": "2024-07-11T14:03:19.914Z"
   },
   {
    "duration": 47,
    "start_time": "2024-07-11T14:03:21.090Z"
   },
   {
    "duration": 107,
    "start_time": "2024-07-11T14:03:21.140Z"
   },
   {
    "duration": 27,
    "start_time": "2024-07-11T14:03:21.249Z"
   },
   {
    "duration": 102,
    "start_time": "2024-07-11T14:03:21.278Z"
   },
   {
    "duration": 9,
    "start_time": "2024-07-11T14:03:21.383Z"
   },
   {
    "duration": 43,
    "start_time": "2024-07-11T14:03:21.412Z"
   },
   {
    "duration": 9,
    "start_time": "2024-07-11T14:03:21.458Z"
   },
   {
    "duration": 57,
    "start_time": "2024-07-11T14:03:21.470Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
